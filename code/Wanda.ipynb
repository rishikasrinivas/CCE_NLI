{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yXa8Iw7ZVfdD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import settings\n",
    "import collections\n",
    "from typing import Union\n",
    "from prune import Pruner\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder_dim = encoder.output_dim\n",
    "        self.mlp_input_dim = self.encoder_dim\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.mlp_input_dim, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(50, 1),\n",
    "        )\n",
    "\n",
    "        self.output_dim = 1\n",
    "\n",
    "    def forward(self, text, length):\n",
    "        enc = self.encoder(text, length)\n",
    "        enc = self.dropout(enc)\n",
    "\n",
    "        logits = self.mlp(enc)\n",
    "        logits = logits.squeeze(1)\n",
    "        return logits\n",
    "\n",
    "    def get_final_reprs(self, text, length):\n",
    "        \"\"\"\n",
    "        Get features right up to final decision\n",
    "        \"\"\"\n",
    "        enc = self.encoder(text, length)\n",
    "        rep = self.mlp[:-1](enc)\n",
    "        return rep\n",
    "\n",
    "\n",
    "class EntailmentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    An NLI entailment classifier where the hidden rep features are much\n",
    "    \"closer\" to the actual feature decision\n",
    "    \"\"\"\n",
    "    #look into how vocab size affects the model. rnn weights are the same beforea fter pruing but vocab size differs. before its 33587 after is 5784 something\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder_dim = encoder.output_dim\n",
    "        self.mlp_input_dim = self.encoder_dim\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bn = nn.BatchNorm1d(self.mlp_input_dim)\n",
    "\n",
    "        self.mlp = nn.Linear(self.mlp_input_dim, 3)\n",
    "\n",
    "        self.output_dim = 3\n",
    "\n",
    "    def forward(self, s1, s1len, s2, s2len):\n",
    "        s1enc = self.encoder(s1, s1len)\n",
    "        s2enc = self.encoder(s2, s2len)\n",
    "\n",
    "        mlp_input = s1enc * s2enc\n",
    "\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "\n",
    "        preds = self.mlp(mlp_input)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def get_final_reprs(self, s1, s1len, s2, s2len):\n",
    "        \"\"\"\n",
    "        Get features right up to final decision\n",
    "        \"\"\"\n",
    "        s1enc = self.encoder(s1, s1len)\n",
    "        s2enc = self.encoder(s2, s2len)\n",
    "        mlp_input = s1enc * s2enc\n",
    "\n",
    "        return mlp_input\n",
    "#https://github.com/lecode-official/pytorch-lottery-ticket-hypothesis/blob/main/source/lth/models/__init__.py\n",
    "class Layer:\n",
    "    \"\"\"Represents a single prunable layer in the neural network.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            name: str,\n",
    "            weights: torch.nn.Parameter,\n",
    "            initial_weights: torch.Tensor,\n",
    "            pruning_mask: torch.Tensor) -> None:\n",
    "        \"\"\"Initializes a new Layer instance.\n",
    "\n",
    "        Args:\n",
    "            name (str): The name of the layer.\n",
    "            kind (LayerKind): The kind of the layer.\n",
    "            weights (torch.nn.Parameter): The weights of the layer.\n",
    "            biases (torch.nn.Parameter): The biases of the layer.\n",
    "            initial_weights (torch.Tensor): A copy of the initial weights of the layer.\n",
    "            initial_biases (torch.Tensor): A copy of the initial biases of the layer.\n",
    "            pruning_mask (torch.Tensor): The current pruning mask of the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name\n",
    "\n",
    "        self.weights = weights\n",
    "        self.initial_weights = initial_weights\n",
    "        self.pruning_mask = pruning_mask\n",
    "\n",
    "# referenced from: https://github.com/lecode-official/pytorch-lottery-ticket-hypothesis/blob/main/source/lth/models/__init__.py\n",
    "class BaseModel(torch.nn.Module):\n",
    "    \"\"\"Represents the base class for all models.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initializes a new BaseModel instance. Since this is a base class, it should never be called directly.\"\"\"\n",
    "\n",
    "        # Invokes the constructor of the base class\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializes some class members\n",
    "        self.layers = None\n",
    "\n",
    "\n",
    "    def initialize(self) -> None:\n",
    "        \"\"\"Initializes the model. It initializes the weights of the model using Xavier Normal (equivalent to Gaussian Glorot used in the original\n",
    "        Lottery Ticket Hypothesis paper). It also creates an initial pruning mask for the layers of the model. These are initialized with all ones. A\n",
    "        pruning mask with all ones does nothing. This method must be called by all sub-classes at the end of their constructor.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Gets the all the fully-connected and convolutional layers of the model (these are the only ones that are being used right now, if new layer\n",
    "        # types are introduced, then they have to be added here, but right now all models only consist of these two types)\n",
    "        self.layers = []\n",
    "        for parameter_name, parameter in self.named_parameters():\n",
    "            weights = parameter\n",
    "\n",
    "            weights.requires_grad = True\n",
    "            init_weights=parameter.clone()\n",
    "\n",
    "            # Initializes the pruning masks of the layer, which are used for pruning as well as freezing the pruned weights during training\n",
    "            pruning_mask = torch.ones_like(init_weights, dtype=torch.uint8).to('cuda')  # pylint: disable=no-member\n",
    "            # Adds the layer to the internal list of layers\n",
    "\n",
    "\n",
    "            self.layers.append(Layer(parameter_name, weights, init_weights, pruning_mask))\n",
    "\n",
    "\n",
    "\n",
    "    def get_layer_names(self):\n",
    "        \"\"\"Retrieves the internal names of all the layers of the model.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Returns a list of all the names of the layers of the model.\n",
    "        \"\"\"\n",
    "\n",
    "        layer_names = []\n",
    "        for layer in self.layers:\n",
    "            layer_names.append(layer.name)\n",
    "        return layer_names\n",
    "\n",
    "    def get_layer(self, layer_name: str) -> Layer:\n",
    "        \"\"\"Retrieves the layer of the model with the specified name.\n",
    "\n",
    "        Args:\n",
    "            layer_name (str): The name of the layer that is to be retrieved.\n",
    "\n",
    "        Raises:\n",
    "            LookupError: If the layer does not exist, an exception is raised.\n",
    "\n",
    "        Returns:\n",
    "            Layer: Returns the layer with the specified name.\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if layer.name == layer_name:\n",
    "                return layer\n",
    "        raise LookupError(f'The specified layer \"{layer_name}\" does not exist.')\n",
    "\n",
    "    def update_layer_weights(self, mask, layer_name: str, new_weights: torch.Tensor) -> None:\n",
    "        \"\"\"Updates the weights of the specified layer.\n",
    "\n",
    "        Args:\n",
    "            layer_name (str): The name of the layer whose weights are to be updated.\n",
    "            new_weights (torch.Tensor): The new weights of the layer.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Update the layer weights\n",
    "            self.state_dict()[layer_name].copy_(new_weights)\n",
    "            self.get_layer(layer_name).weights.copy_(new_weights)\n",
    "\n",
    "            self.get_layer(layer_name).pruning_mask.copy_(mask)\n",
    "\n",
    "    def get_total_num_weights(self):\n",
    "        terms =0\n",
    "        for l in self.layers:\n",
    "            l = self.get_layer(l.name)\n",
    "            terms += l.weights.flatten().shape[0]\n",
    "        return terms\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Resets the model back to its initial initialization.\"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            self.state_dict()[f'{layer.name}.weight'].copy_(layer.initial_weights)\n",
    "\n",
    "    def move_to_device(self, device: Union[int, str, torch.device]) -> None:  # pylint: disable=no-member\n",
    "        \"\"\"Moves the model to the specified device.\n",
    "\n",
    "        Args:\n",
    "            device (Union[int, str, torch.device]): The device that the model is to be moved to.\n",
    "        \"\"\"\n",
    "\n",
    "        # Moves the model itself to the device\n",
    "        self.to(device)\n",
    "\n",
    "        # Moves the initial weights, initial biases, and the pruning masks also to the device\n",
    "        for layer in self.layers:\n",
    "            layer.initial_weights = layer.initial_weights.to(device)\n",
    "            layer.pruning_mask = layer.pruning_mask.to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs the forward pass through the neural network. Since this is the base model, the method is not implemented and must be implemented\n",
    "        in all classes that derive from the base model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input to the neural network.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: _description_\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Returns the output of the neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class BertEntailmentClassifier(BaseModel):\n",
    "    def __init__(self, encoder_name=\"bert-base-uncased\", vocab=None, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "\n",
    "#         if freeze_bert:\n",
    "#             for param in self.encoder.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        self.mlp_input_dim = self.encoder_dim * 4\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bn = nn.BatchNorm1d(self.mlp_input_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.mlp_input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 3),\n",
    "        )\n",
    "        self.output_dim = 3\n",
    "        self.initialize()\n",
    "\n",
    "        self.p=Pruner(self)\n",
    "\n",
    "    def forward(self, s1, s1len, s2, s2len):\n",
    "        device = s1.device\n",
    "\n",
    "        s1 = s1.transpose(1, 0)\n",
    "        s2 = s2.transpose(1, 0)\n",
    "\n",
    "        s1_tokens = self.indices_to_bert_tokens(s1)\n",
    "        s2_tokens = self.indices_to_bert_tokens(s2)\n",
    "\n",
    "        s1_tokens = {k: v.to(device) for k, v in s1_tokens.items()}\n",
    "        s2_tokens = {k: v.to(device) for k, v in s2_tokens.items()}\n",
    "\n",
    "        s1enc = self.encode_sentence(s1_tokens)\n",
    "        s2enc = self.encode_sentence(s2_tokens)\n",
    "\n",
    "        diffs = s1enc - s2enc\n",
    "        prods = s1enc * s2enc\n",
    "\n",
    "        mlp_input = torch.cat([s1enc, s2enc, diffs, prods], 1)\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "        preds = self.mlp(mlp_input)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def get_final_reprs(self, s1, s1len, s2, s2len):\n",
    "        device = s1.device\n",
    "\n",
    "        s1 = s1.transpose(1, 0)\n",
    "        s2 = s2.transpose(1, 0)\n",
    "\n",
    "        s1_tokens = self.indices_to_bert_tokens(s1)\n",
    "        s2_tokens = self.indices_to_bert_tokens(s2)\n",
    "\n",
    "        s1_tokens = {k: v.to(device) for k, v in s1_tokens.items()}\n",
    "        s2_tokens = {k: v.to(device) for k, v in s2_tokens.items()}\n",
    "\n",
    "        s1enc = self.encode_sentence(s1_tokens)\n",
    "        s2enc = self.encode_sentence(s2_tokens)\n",
    "\n",
    "        diffs = s1enc - s2enc\n",
    "        prods = s1enc * s2enc\n",
    "\n",
    "        mlp_input = torch.cat([s1enc, s2enc, diffs, prods], 1)\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "        rep = self.mlp[:-1](mlp_input)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def forward_from_final(self, rep):\n",
    "        preds = self.mlp[-1:](rep)\n",
    "        return preds\n",
    "\n",
    "    def indices_to_bert_tokens(self, indices):\n",
    "        batch_size, seq_len = indices.shape\n",
    "        words = []\n",
    "        for i in range(batch_size):\n",
    "            sentence = []\n",
    "            for idx in indices[i]:\n",
    "                if idx.item() in self.vocab['itos']:\n",
    "                    word = self.vocab['itos'][idx.item()]\n",
    "                    if word not in (\"[PAD]\", \"<pad>\", \"PAD\"):\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    break\n",
    "            words.append(sentence)\n",
    "\n",
    "        return self.tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    def encode_sentence(self, tokens):\n",
    "        outputs = self.encoder(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    def to(self, device):\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    def prune(self):\n",
    "        self.p.prune()\n",
    "        return self\n",
    "\n",
    "\n",
    "class BowmanEntailmentClassifier(BaseModel):\n",
    "    \"\"\"\n",
    "    The RNN-based entailment model of Bowman et al 2017\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.encoder_dim = encoder.output_dim\n",
    "        self.mlp_input_dim = self.encoder_dim * 4\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bn = nn.BatchNorm1d(self.mlp_input_dim)\n",
    "        self.prune_mask= torch.ones(1024,self.mlp_input_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.mlp_input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),  # Mimic classifier MLP keep rate of 94%\n",
    "            nn.Linear(1024, 3),\n",
    "        )\n",
    "        #self.mlp[:-1][0] = prune.ln_structured(self.mlp[:-1][0], name=\"weight\", amount=0.05, dim=1, n=float('-inf'))\n",
    "        self.output_dim = 3\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "        self.p=Pruner(self)\n",
    "\n",
    "\n",
    "    def forward(self, s1, s1len, s2, s2len):\n",
    "        s1enc = self.encoder(s1, s1len)\n",
    "        s2enc = self.encoder(s2, s2len)\n",
    "\n",
    "\n",
    "        diffs = s1enc - s2enc\n",
    "        prods = s1enc * s2enc\n",
    "\n",
    "        mlp_input = torch.cat([s1enc, s2enc, diffs, prods], 1) #1x2048\n",
    "\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "\n",
    "        preds = self.mlp(mlp_input)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def check_pruned(self, layer='default'):\n",
    "        if layer == 'default':\n",
    "            layer = self.mlp[:-1]\n",
    "        return prune.is_pruned(layer)\n",
    "    def prune(self):\n",
    "        self.p.prune()\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # from https://github.com/jankrepl/mildlyoverfitted/blob/master/github_adventures/lottery/utils.py\n",
    "    def copy_weights_linear(linear_unpruned, linear_pruned):\n",
    "        \"\"\"Copy weights from an unpruned model to a pruned model.\n",
    "\n",
    "        Modifies `linear_pruned` in place.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        linear_unpruned : nn.Linear\n",
    "            Linear model with a bias that was not pruned.\n",
    "\n",
    "        linear_pruned : nn.Linear\n",
    "            Linear model with a bias that was pruned.\n",
    "        \"\"\"\n",
    "        assert check_pruned_linear(linear_pruned)\n",
    "        assert not check_pruned_linear(linear_unpruned)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            linear_pruned.weight_orig.copy_(linear_unpruned.weight)\n",
    "            linear_pruned.bias_orig.copy_(linear_unpruned.bias)\n",
    "\n",
    "    def get_final_reprs(self, s1, s1len, s2, s2len):\n",
    "        s1enc = self.encoder(s1, s1len)\n",
    "        s2enc = self.encoder(s2, s2len)\n",
    "\n",
    "        diffs = s1enc - s2enc\n",
    "        prods = s1enc * s2enc\n",
    "\n",
    "        mlp_input = torch.cat([s1enc, s2enc, diffs, prods], 1)\n",
    "\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "\n",
    "\n",
    "        rep = self.mlp[:-1](mlp_input)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def forward_from_final(self, rep):\n",
    "        preds = self.mlp[-1:](rep)\n",
    "        return preds\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder\n",
    "\n",
    "\n",
    "class BertEntailmentClassifier(BaseModel):\n",
    "    def __init__(self, encoder_name=\"bert-base-uncased\", vocab=None, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.encoder_name = encoder_name\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(encoder_name)\n",
    "\n",
    "#         if freeze_bert:\n",
    "#             for param in self.encoder.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "        self.encoder_dim = self.encoder.config.hidden_size\n",
    "        self.mlp_input_dim = self.encoder_dim * 4\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.bn = nn.BatchNorm1d(self.mlp_input_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.mlp_input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 3),\n",
    "        )\n",
    "        self.output_dim = 3\n",
    "        self.initialize()\n",
    "\n",
    "        self.p=Pruner(self)\n",
    "\n",
    "    def forward(self, s1, s1len, s2, s2len):\n",
    "        device = s1.device\n",
    "\n",
    "        s1 = s1.transpose(1, 0)\n",
    "        s2 = s2.transpose(1, 0)\n",
    "\n",
    "        s1_tokens = self.indices_to_bert_tokens(s1)\n",
    "        s2_tokens = self.indices_to_bert_tokens(s2)\n",
    "\n",
    "        s1_tokens = {k: v.to(device) for k, v in s1_tokens.items()}\n",
    "        s2_tokens = {k: v.to(device) for k, v in s2_tokens.items()}\n",
    "\n",
    "        s1enc = self.encode_sentence(s1_tokens)\n",
    "        s2enc = self.encode_sentence(s2_tokens)\n",
    "\n",
    "        diffs = s1enc - s2enc\n",
    "        prods = s1enc * s2enc\n",
    "\n",
    "        mlp_input = torch.cat([s1enc, s2enc, diffs, prods], 1)\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "        preds = self.mlp(mlp_input)\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def get_final_reprs(self, s1, s1len, s2, s2len):\n",
    "        device = s1.device\n",
    "\n",
    "        s1 = s1.transpose(1, 0)\n",
    "        s2 = s2.transpose(1, 0)\n",
    "\n",
    "        s1_tokens = self.indices_to_bert_tokens(s1)\n",
    "        s2_tokens = self.indices_to_bert_tokens(s2)\n",
    "\n",
    "        s1_tokens = {k: v.to(device) for k, v in s1_tokens.items()}\n",
    "        s2_tokens = {k: v.to(device) for k, v in s2_tokens.items()}\n",
    "\n",
    "        s1enc = self.encode_sentence(s1_tokens)\n",
    "        s2enc = self.encode_sentence(s2_tokens)\n",
    "\n",
    "        diffs = s1enc - s2enc\n",
    "        prods = s1enc * s2enc\n",
    "\n",
    "        mlp_input = torch.cat([s1enc, s2enc, diffs, prods], 1)\n",
    "        mlp_input = self.bn(mlp_input)\n",
    "        mlp_input = self.dropout(mlp_input)\n",
    "        rep = self.mlp[:-1](mlp_input)\n",
    "\n",
    "        return rep\n",
    "\n",
    "    def forward_from_final(self, rep):\n",
    "        preds = self.mlp[-1:](rep)\n",
    "        return preds\n",
    "\n",
    "    def indices_to_bert_tokens(self, indices):\n",
    "        batch_size, seq_len = indices.shape\n",
    "        words = []\n",
    "        for i in range(batch_size):\n",
    "            sentence = []\n",
    "            for idx in indices[i]:\n",
    "                if idx.item() in self.vocab['itos']:\n",
    "                    word = self.vocab['itos'][idx.item()]\n",
    "                    if word not in (\"[PAD]\", \"<pad>\", \"PAD\"):\n",
    "                        sentence.append(word)\n",
    "                else:\n",
    "                    break\n",
    "            words.append(sentence)\n",
    "\n",
    "        return self.tokenizer(words, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    def encode_sentence(self, tokens):\n",
    "        outputs = self.encoder(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "    def to(self, device):\n",
    "        self.encoder = self.encoder.to(device)\n",
    "        return super().to(device)\n",
    "\n",
    "    def prune(self):\n",
    "        self.p.prune()\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DropoutLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W_i = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.U_i = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.U_f = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.W_c = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.U_c = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.W_o = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.U_o = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self._input_dropout_mask = self._h_dropout_mask = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.orthogonal_(self.W_i)\n",
    "        nn.init.orthogonal_(self.U_i)\n",
    "        nn.init.orthogonal_(self.W_f)\n",
    "        nn.init.orthogonal_(self.U_f)\n",
    "        nn.init.orthogonal_(self.W_o)\n",
    "        nn.init.orthogonal_(self.U_o)\n",
    "        nn.init.orthogonal_(self.W_c)\n",
    "        nn.init.orthogonal_(self.U_c)\n",
    "        self.b_f.data.fill_(1.0)\n",
    "        self.b_i.data.fill_(1.0)\n",
    "        self.b_o.data.fill_(1.0)\n",
    "\n",
    "    def set_dropout_masks(self, batch_size):\n",
    "        if self.dropout:\n",
    "            if self.training:\n",
    "                self._input_dropout_mask = torch.bernoulli(\n",
    "                    torch.Tensor(4, batch_size, self.input_size).fill_(1 - self.dropout)\n",
    "                )\n",
    "                self._input_dropout_mask.requires_grad = False\n",
    "                self._h_dropout_mask = torch.bernoulli(\n",
    "                    torch.Tensor(4, batch_size, self.hidden_size).fill_(\n",
    "                        1 - self.dropout\n",
    "                    )\n",
    "                )\n",
    "                self._h_dropout_mask.requires_grad = False\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    self._input_dropout_mask = self._input_dropout_mask.cuda()\n",
    "                    self._h_dropout_mask = self._h_dropout_mask.cuda()\n",
    "            else:\n",
    "                self._input_dropout_mask = self._h_dropout_mask = [\n",
    "                    1.0 - self.dropout\n",
    "                ] * 4\n",
    "        else:\n",
    "            self._input_dropout_mask = self._h_dropout_mask = [1.0] * 4\n",
    "\n",
    "    def forward(self, input, hidden_state):\n",
    "        h_tm1, c_tm1 = hidden_state\n",
    "\n",
    "        if self._input_dropout_mask is None:\n",
    "            self.set_dropout_masks(input.size(0))\n",
    "\n",
    "        xi_t = F.linear(input * self._input_dropout_mask[0], self.W_i, self.b_i)\n",
    "        xf_t = F.linear(input * self._input_dropout_mask[1], self.W_f, self.b_f)\n",
    "        xc_t = F.linear(input * self._input_dropout_mask[2], self.W_c, self.b_c)\n",
    "        xo_t = F.linear(input * self._input_dropout_mask[3], self.W_o, self.b_o)\n",
    "\n",
    "        i_t = F.sigmoid(xi_t + F.linear(h_tm1 * self._h_dropout_mask[0], self.U_i))\n",
    "        f_t = F.sigmoid(xf_t + F.linear(h_tm1 * self._h_dropout_mask[1], self.U_f))\n",
    "        c_t = f_t * c_tm1 + i_t * F.tanh(\n",
    "            xc_t + F.linear(h_tm1 * self._h_dropout_mask[2], self.U_c)\n",
    "        )\n",
    "        o_t = F.sigmoid(xo_t + F.linear(h_tm1 * self._h_dropout_mask[3], self.U_o))\n",
    "        h_t = o_t * F.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim=300, hidden_dim=512, bidirectional=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=1)\n",
    "        self.rnn = nn.LSTM(\n",
    "            self.embedding_dim, self.hidden_dim, bidirectional=bidirectional\n",
    "        )\n",
    "        self.output_dim = self.hidden_dim\n",
    "\n",
    "    def forward(self, s, slen):\n",
    "        semb = self.emb(s)\n",
    "        spk = pack_padded_sequence(semb, slen.cpu(), enforce_sorted=False)\n",
    "        _, (hidden, cell) = self.rnn(spk)\n",
    "\n",
    "        #retunr get all cell states w a param for the cell state #\n",
    "        return hidden[-1]\n",
    "\n",
    "\n",
    "    def get_states(self, s, slen):\n",
    "        semb = self.emb(s)\n",
    "        spk = pack_padded_sequence(semb, slen.cpu(), enforce_sorted=False)\n",
    "        outputs, _ = self.rnn(spk)\n",
    "        print(outputs)\n",
    "        outputs_pad = pad_packed_sequence(outputs)[0]\n",
    "        return outputs_pad #padded hidden states for each word\n",
    "\n",
    "    def get_last_cell_state(self, s,slen):\n",
    "        semb = self.emb(s)\n",
    "        spk = pack_padded_sequence(semb, slen.cpu(), enforce_sorted=False)\n",
    "        _, (hidden, cell) = self.rnn(spk)\n",
    "\n",
    "\n",
    "        return cell[-1]\n",
    "\n",
    "\n",
    "class DropoutTextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim=300, hidden_dim=512, bidirectional=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=1)\n",
    "        self.rnn_cell = DropoutLSTMCell(\n",
    "            self.embedding_dim, self.hidden_dim, dropout=0.5\n",
    "        )\n",
    "        self.output_dim = self.hidden_dim\n",
    "\n",
    "    def forward(self, s, slen):\n",
    "        semb = self.emb(s)\n",
    "\n",
    "        hx = torch.zeros(semb.shape[1], self.hidden_dim).to(semb.device)\n",
    "        cx = torch.zeros(semb.shape[1], self.hidden_dim).to(semb.device)\n",
    "        for i in range(semb.shape[0]):\n",
    "            hx, cx = self.rnn_cell(semb[i], (hx, cx))\n",
    "        return hx\n",
    "\n",
    "    def get_states(self, s, slen):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/CCE_NLI/code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cnr4gf77Vx1X"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/DataLoaders/train_dataset.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SNLI, pad_collate\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models/DataLoaders/train_dataset.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      5\u001b[0m     train_dataset,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mpad_collate\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m vocab\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoi\u001b[39m\u001b[38;5;124m'\u001b[39m: train_dataset\u001b[38;5;241m.\u001b[39mstoi, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitos\u001b[39m\u001b[38;5;124m'\u001b[39m: train_dataset\u001b[38;5;241m.\u001b[39mitos}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:786\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    784\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 786\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/DataLoaders/train_dataset.pth'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from data.snli import SNLI, pad_collate\n",
    "train_dataset = torch.load(f'../models/DataLoaders/train_dataset.pth')\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=True,\n",
    "    pin_memory=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=pad_collate\n",
    ")\n",
    "vocab={'stoi': train_dataset.stoi, 'itos': train_dataset.itos}\n",
    "model_bert=BertEntailmentClassifier(vocab=vocab)\n",
    "prepare_calibration_input(model_bert, train_loader, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGKYgm_qVlZZ"
   },
   "outputs": [],
   "source": [
    "def find_final_layers(module, layers=[nn.Linear], name=''):\n",
    "    \"\"\"\n",
    "    Recursively find the layers of a certain type in a module.\n",
    "\n",
    "    Args:\n",
    "        module (nn.Module): PyTorch module.\n",
    "        layers (list): List of layer types to find.\n",
    "        name (str): Name of the module.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of layers of the given type(s) within the module.\n",
    "    \"\"\"\n",
    "    if type(module) in layers:\n",
    "        return {name: module}\n",
    "    res = {}\n",
    "    for name1, child in module.named_children():\n",
    "        res.update(find_layers(\n",
    "            child, layers=layers, name=name + '.' + name1 if name != '' else name1\n",
    "        ))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSuLvsDsVmnG"
   },
   "outputs": [],
   "source": [
    "def prepare_calibration_input(model, dataloader, device):\n",
    "\n",
    "    layers = model.model.layers\n",
    "\n",
    "    # dev = model.hf_device_map[\"model.embed_tokens\"]\n",
    "\n",
    "    dtype = next(iter(model.parameters())).dtype\n",
    "    inps = torch.zeros((128, model.seqlen, 305227), dtype=dtype, device=device)\n",
    "    inps.requires_grad = False\n",
    "    cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "    class Catcher(nn.Module):\n",
    "        def __init__(self, module):\n",
    "            super().__init__()\n",
    "            self.module = module\n",
    "        def forward(self, inp, **kwargs):\n",
    "            inps[cache['i']] = inp\n",
    "            cache['i'] += 1\n",
    "            cache['attention_mask'] = kwargs['attention_mask']\n",
    "            cache['position_ids'] = kwargs['position_ids']\n",
    "            raise ValueError\n",
    "    layers[0] = Catcher(layers[0])\n",
    "    for batch in dataloader:\n",
    "        try:\n",
    "            model(batch[0].to(device))\n",
    "        except ValueError:\n",
    "            pass\n",
    "        break\n",
    "    layers[0] = layers[0].module\n",
    "\n",
    "    outs = torch.zeros_like(inps)\n",
    "    attention_mask = cache['attention_mask']\n",
    "    position_ids = cache['position_ids']\n",
    "\n",
    "    return inps, outs, attention_mask, position_ids\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataloader, _ = get_loaders(\"c4\",nsamples=128,seed=0,seqlen=5,tokenizer=tokenizer)\n",
    "\n",
    "model_bert=BertEntailmentClassifier(vocab=vocab)\n",
    "prepare_calibration_input(model, dataloader, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCxbsPoiVpH_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "def get_llm(model_name, cache_dir=\"llm_weights\"):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        cache_dir=cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    model.seqlen = model.config.max_position_embeddings\n",
    "    return model\n",
    "\n",
    "model=get_llm(model_name='baffo32/decapoda-research-llama-7B-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy7oJiRcVqSw"
   },
   "outputs": [],
   "source": [
    "inps = torch.zeros((100,100, 3072), dtype=dtype, device='cuda')\n",
    "inps.requires_grad = False\n",
    "cache = {'i': 0, 'attention_mask': None, \"position_ids\": None}\n",
    "\n",
    "class Catcher(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "    def forward(self, inp, **kwargs):\n",
    "        inps[cache['i']] = inp\n",
    "\n",
    "        cache['i'] += 1\n",
    "        cache['attention_mask'] = kwargs['attention_mask']\n",
    "        cache['position_ids'] = kwargs['position_ids']\n",
    "        print(inps)\n",
    "\n",
    "layers=model_bert.get_layer('mlp.0.weight')\n",
    "print(layers)\n",
    "model_bert.mlp[0] = Catcher(model_bert.get_layer('mlp.0.weight'))\n",
    "for s1,s1l,s2,s2l,t in train_loader:\n",
    "    try:\n",
    "        model_bert(s1,s1l,s2,s2l)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyPjC4GnVrXH"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SNLI dataset\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    We don't sort here to take advantage of enforce_sorted=False since we'd\n",
    "    have to sort separately for both s1 and s2\n",
    "    \"\"\"\n",
    "    s1, s1len, s2, s2len, label = zip(*batch)\n",
    "    label = torch.tensor(label)\n",
    "\n",
    "    s1_pad = pad_sequence(s1, padding_value=1)\n",
    "    s1len = torch.tensor(s1len)\n",
    "\n",
    "    s2_pad = pad_sequence(s2, padding_value=1)\n",
    "    s2len = torch.tensor(s2len)\n",
    "\n",
    "    return s1_pad, s1len, s2_pad, s2len, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ov6vVEsnVzPX"
   },
   "outputs": [],
   "source": [
    "# Code adapted from https://github.com/IST-DASLab/sparsegpt/blob/master/datautils.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "# Wrapper for tokenized input IDs\n",
    "class TokenizerWrapper:\n",
    "    def __init__(self, input_ids):\n",
    "        self.input_ids = input_ids\n",
    "\n",
    "# Load and process c4 dataset\n",
    "def get_c4(nsamples, seed, seqlen, tokenizer):\n",
    "    # Load train and validation datasets\n",
    "    traindata = load_dataset('allenai/c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')\n",
    "    valdata = load_dataset('allenai/c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')\n",
    "\n",
    "    # Generate samples from training set\n",
    "    random.seed(seed)\n",
    "    trainloader = []\n",
    "    for _ in range(nsamples):\n",
    "        while True:\n",
    "            i = random.randint(0, len(traindata) - 1)\n",
    "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
    "            if trainenc.input_ids.shape[1] > seqlen:\n",
    "                break\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        tar = inp.clone()\n",
    "        tar[:, :-1] = -100\n",
    "        trainloader.append((inp, tar))\n",
    "\n",
    "    # Prepare validation dataset\n",
    "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
    "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
    "    valenc = TokenizerWrapper(valenc)\n",
    "    return trainloader, valenc\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataloader, _ = get_loaders(\"c4\",nsamples=128,seed=0,seqlen=5,tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2G2BRa0vV0oD"
   },
   "outputs": [],
   "source": [
    "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):\n",
    "    if 'wikitext2' in name:\n",
    "        return get_wikitext2(nsamples, seed, seqlen, tokenizer)\n",
    "    if \"c4\" in name:\n",
    "        return get_c4(nsamples, seed, seqlen, tokenizer)\n",
    "    if \"snli\" in name:\n",
    "        return get_snli(nsamples, seed, seqlen, tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
